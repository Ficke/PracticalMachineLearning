---
title: "Practical Machine Learning Course Project"
author: "Adam Ficke"
date: "6/20/2020"
output: 
  html_document: 
    df_print: kable
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

require(caret)
require(dplyr)
require(gbm)
require(ggplot2)
require(hablar)
require(doParallel)
require(cvms)
require(rsvg)
require(ggimage)
require(knitr)
require(magrittr)
require(kableExtra)

```

```{r read in data, include=FALSE}
#read in data
data <-
  read.csv(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
    stringsAsFactors = FALSE
  )
test <-
  read.csv(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
    stringsAsFactors = FALSE
  )
```

## Problem Statement

This project aims to classify users' exercise technique using data from a personal fitness device. 

This paper will discuss the methods used for explotatory data analysis, feature selection, model selection, and performance evaluation, and will ultimately show modeled results on an independent test dataset. 

All data in this project is sourced from http://groupware.les.inf.puc-rio.br/har

Creative Commons license (CC BY-SA)

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.*

## EDA

First we'll examine the data for zero or near zero variance so that we can remove them from our predictors.


```{r clean data, echo=FALSE, message=FALSE, warning=FALSE}

nzvList <- nearZeroVar(data, saveMetrics = TRUE)
knitr::kable(nzvList[1:10,], align = "c")%>%
  kable_styling(full_width = F)

```

```{r nzd1, include=FALSE}
nzv <- nearZeroVar(data)

FilteredData <- data[, -nzv]
FilteredTest <- test[, -nzv]
```


We'll also search for features that have mostly invalid data, and filter those out. 


```{r NAs, echo=TRUE}
zeros <- colMeans(!is.na(FilteredData)) > .97
FilteredData <- FilteredData[zeros]
FilteredTest <- FilteredTest[zeros]
zs<-as.matrix(summary(zeros))
knitr::kable(zs, align = "c")%>%
  kable_styling(full_width = F)
```


Next we'll filter out the variables that don't make sense as predictors, such as the date. 


```{r variable reduction 1, echo=TRUE}
names(FilteredData[,1:10])

FilteredData <- FilteredData[, -c(1, 3, 4, 5, 6)]
FilteredTest <- FilteredTest[-c(1, 3, 4, 5, 6)]
```


Now we need to standardize the variables types. First we'll review what the types currently are for the training and test sets, then change them all to correspond with an appropriate type. 


```{r variable type, echo=FALSE}

data.frame(variable = names(FilteredData),
           classe = sapply(FilteredData, typeof),
           first_values = sapply(FilteredData, function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) %>% 
  kable()

#standardize data types
FilteredData[, 2:53] <- 
  lapply(FilteredData[, 2:53], as.numeric)
FilteredData[, c(1, 54)] <-
  lapply(FilteredData[, c(1, 54)], as.factor)

FilteredTest[, 2:53] <- 
  lapply(FilteredTest[, 2:53], as.numeric)
FilteredTest[, c(1, 54)] <-
  lapply(FilteredTest[, c(1, 54)], as.factor)
```



Now that the data is clean, we'll split out our training set into a 70/30 train/test split. 



```{r training test split, echo=TRUE}
set.seed(250)
trainIndex <- createDataPartition(FilteredData$classe, p = .7, 
                                  list = FALSE, 
                                  times = 1)

train <- FilteredData[trainIndex, ]
valid <- FilteredData[-trainIndex, ]
```


## Model Selection


Since these are classification problems where the interpretability of the model is less important than the accuracy, we'll try a couple of machine learning models: GBM and Random Forest. 

These models have a tendancy to overfit, so we'll make sure to use cross-validation. 


### GBM

```{r GBM, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
akePSOCKcluster(8)
registerDoParallel(cl)

fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10)


set.seed(250)
gbmFit1 <- train(
  classe ~ .,
  data = train,
  method = "gbm",
  trControl = fitControl,
  verbose = FALSE,
  na.action = na.pass
)
stopCluster(cl)
```


```{r GBM display, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

summary(gbmFit1)
varImp(gbmFit1)

```

```{r GBM Output, echo=FALSE, message=FALSE, warning=FALSE}

gbmFit<-readRDS(file = "GBM1.rds")

gbmImp<-varImp(gbmFit)
plot(gbmImp, top = 20)
```


### Random Forest

```{r RF, eval=FALSE, echo = TRUE}
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

set.seed(250)
rfFit1 <- train(
  classe ~ .,
  data = train,
  method = "rf",
  trControl = fitControl,
  na.action = na.pass
)

stopCluster(cl)
registerDoSEQ()
```



```{r RF Output, echo=FALSE, message=FALSE, warning=FALSE}

rfFit<-readRDS(file = "RF1.rds")
rfImp<-varImp(rfFit)
plot(rfImp, top = 20)

```


## Performance


Now we'll look at the confusion matrix to assess the accuracy of each model. 


### GBM Performance 

```{r performance GBM, echo=FALSE}

predictions.gbm <- predict(gbmFit,valid,na.action = na.pass)

conf.mat.gbm <- confusion_matrix(targets = valid$classe,
                             predictions = predictions.gbm)

confusionMatrix(data = predictions.gbm, reference = valid$classe)
plot_confusion_matrix(conf.mat.gbm$`Confusion Matrix`[[1]])

```


### Random Forest Performance 

```{r performance RF, echo=FALSE}
predictions.rf <- predict(rfFit,valid,na.action = na.pass)

conf_mat <- confusion_matrix(targets = valid$classe,
                             predictions = predictions.rf)

confusionMatrix(data = predictions.rf, reference = valid$classe)
plot_confusion_matrix(conf_mat$`Confusion Matrix`[[1]])

```


Both models are performing well, though the Random Forest has a slightly better accuracy of 99.4% vs. 96.2% for the GBM. 


## Predictions


The last step is to use our models to predict on unknown quantities. Since both models seem to perform well, we'll run predictions using each and compare the results. In this case All of the predictions agree, which gives more confidence in the results. 


```{r Predictions, echo=FALSE}
#predict on 20 data points 
predictions.test.rf <- predict(rfFit,test,na.action = na.pass)
predictions.test2.gbm <- predict(gbmFit,test,na.action = na.pass)
pred<-data.frame(
  ProblemID = seq(1:20),
  GBM_Predictions = predictions.test2.gbm,
  Random_Forest_Predictions = predictions.test.rf
)

knitr::kable(pred, align = "c")%>%
  kable_styling(full_width = F)

```
