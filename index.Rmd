---
title: "Practical Machine Learning Course Project"
author: "Adam Ficke"
date: "6/20/2020"
output: 
  html_document: 
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

require(caret)
require(dplyr)
require(gbm)
require(ggplot2)
require(hablar)
require(doParallel)
require(cvms)
require(rsvg)
require(ggimage)
require(knitr)
require(magrittr)

```

```{r read in data, include=FALSE}
#read in data
data <-
  read.csv(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
    stringsAsFactors = FALSE
  )
test <-
  read.csv(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
    stringsAsFactors = FALSE
  )
```

## Problem Statement

This project aims to classify exercise technique using personal fitness device data. I'll discuss the methods used for explotatory data analysis, feature selection, model selection, and performance evaluation.

All data in this project is sourced from http://groupware.les.inf.puc-rio.br/har

Creative Commons license (CC BY-SA)

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.*

### EDA

First we'll examine the data for zero or near zero variance so that we can remove them. 

```{r clean data, echo=TRUE, message=TRUE, warning=FALSE}

nzvList <- nearZeroVar(data, saveMetrics = TRUE)
nzvList[1:10,]
```

```{r nzd1, include=FALSE}
nzv <- nearZeroVar(data)

FilteredData <- data[, -nzv]
FilteredTest <- test[, -nzv]
```

```{r NAs, echo=TRUE}
zeros <- colMeans(!is.na(FilteredData)) > .97
FilteredData <- FilteredData[zeros]
FilteredTest <- FilteredTest[zeros]
as.table(summary(zeros))
```


Next we'll filter out the variables that don't make sense as predictors, such as the date. 

```{r variable reduction 1, echo=TRUE}
names(FilteredData[,1:10])

FilteredData <- FilteredData[, -c(1, 3, 4, 5, 6)]
FilteredTest <- FilteredTest[-c(1, 3, 4, 5, 6)]
```

Now we need to standardize the variables types. First we'll review what the types currently are for the training and test sets, then change them all to correspond with an appropriate type. 

```{r variable type, echo=TRUE}

data.frame(variable = names(FilteredData),
           classe = sapply(FilteredData, typeof),
           first_values = sapply(FilteredData, function(x) paste0(head(x),  collapse = ", ")),
           row.names = NULL) %>% 
  kable()

#standardize data types
FilteredData[, 2:53] <- 
  lapply(FilteredData[, 2:53], as.numeric)
FilteredData[, c(1, 54)] <-
  lapply(FilteredData[, c(1, 54)], as.factor)

FilteredTest[, 2:53] <- 
  lapply(FilteredTest[, 2:53], as.numeric)
FilteredTest[, c(1, 54)] <-
  lapply(FilteredTest[, c(1, 54)], as.factor)
```

Now that the data is clean, we'll split out our training set into a 70/30 train/test split. 

```{r training test split, include=FALSE}
set.seed(250)
trainIndex <- createDataPartition(FilteredData$classe, p = .7, 
                                  list = FALSE, 
                                  times = 1)

train <- FilteredData[trainIndex, ]
valid <- FilteredData[-trainIndex, ]
```


## Model Selection


Since these are classification problems where the interpretability of the model is less important than the accuracy, we'll try a couple of machine learning models: GBM and Random Forest. 

These models have a tendancy to overfit, so we'll make sure to use cross-validation. 


```{r GBM, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
akePSOCKcluster(8)
registerDoParallel(cl)

fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10)


set.seed(250)
gbmFit1 <- train(
  classe ~ .,
  data = train,
  method = "gbm",
  trControl = fitControl,
  verbose = FALSE,
  na.action = na.pass
)
stopCluster(cl)
```

```{r GBM display, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

summary(gbmFit1)
varImp(gbmFit1)

```


```{r RF, eval=FALSE, include=FALSE}
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

set.seed(250)
rfFit1 <- train(
  classe ~ .,
  data = train,
  method = "rf",
  trControl = fitControl,
  na.action = na.pass
)

stopCluster(cl)
registerDoSEQ()
```

```{r GBM Output, echo=FALSE, message=FALSE, warning=FALSE}

gbmFit<-readRDS(file = "GBM1.rds")
varImp(gbmFit)
```

```{r RF Output, echo=FALSE, message=FALSE, warning=FALSE}

rfFit<-readRDS(file = "RF1.rds")
varImp(rfFit)

```


## Performance

```{r performance GBM, echo=TRUE}

predictions.gbm <- predict(gbmFit,valid,na.action = na.pass)

conf.mat.gbm <- confusion_matrix(targets = valid$classe,
                             predictions = predictions.gbm)

confusionMatrix(data = predictions.gbm, reference = valid$classe)
plot_confusion_matrix(conf.mat.gbm$`Confusion Matrix`[[1]])

```

```{r performance RF, echo=TRUE}
predictions.rf <- predict(rfFit,valid,na.action = na.pass)

conf_mat <- confusion_matrix(targets = valid$classe,
                             predictions = predictions.rf)

confusionMatrix(data = predictions.rf, reference = valid$classe)
plot_confusion_matrix(conf_mat$`Confusion Matrix`[[1]])

```


## Predictions


The last step is to use our models to predict on unknown quantities. Since both models seem to perform well, we'll run predictions using each and compare the results. 

```{r Predictions, echo=TRUE}
#predict on 20 data points 
predictions.test <- predict(rfFit,test,na.action = na.pass)
predictions.test

predictions.test2 <- predict(gbmFit,test,na.action = na.pass)
predictions.test2
```

All of the predictions agree, which gives more confidence in the results. 